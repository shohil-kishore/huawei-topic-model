{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "179e22ad",
   "metadata": {},
   "source": [
    "# Docs\n",
    "\n",
    "This jupyter notebook is used to extract Huawei-related tweets for four different countries, AUS, UK, CAN and USA, with the goal of identifying which countries had the most similar conversations six months prior to their respective bans. Process below:\n",
    "\n",
    "1. It cleans data over multiple iterations of identifying irrelevant terms\n",
    "2. Extracts tweets by country using a set method (identifying top five countries and iterating until done)\n",
    "3. Uses unique windows of times to identify six months of tweets prior to the ban\n",
    "4. Processes the topic model for each of the countries individually with a unique seed to define how many topics every country will be restricted to (30)\n",
    "5. Recalculates topic models with the same seed\n",
    "6. Creates a DF with similarity scores based on each topic models embeddings \n",
    "7. Creates this DF for each country\n",
    "7. And, finally, identifies which countries similarity scores are highest, utlimately identifying which countries had the most similar conversations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ef4c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from platform import python_version\n",
    "print(python_version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84771cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import relevant packages. \n",
    "import pandas as pd\n",
    "from bertopic import BERTopic\n",
    "from ipywidgets import FloatProgress\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction import text\n",
    "import gensim.corpora as corpora\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "import pickle\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import datetime\n",
    "from datetime import date\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "from umap import UMAP\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from bertopic import BERTopic\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from umap import UMAP\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Set relevant options.\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.options.display.float_format = '{:.4f}'.format\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70318b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data.\n",
    "combined = pd.read_csv(r\".\\huawei-v2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b199a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb2b1b3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Quality check.\n",
    "print(len(combined))\n",
    "combined = combined.drop_duplicates(subset=['tweet_id'])\n",
    "print(len(combined))\n",
    "\n",
    "# Drop tweets with irrelevant terms.\n",
    "print(len(combined))\n",
    "combined = combined[~combined['cleaned_text'].str.contains(\"giveaway|foldable|p30|mate|p20|charging|apple|ios|iphone|samsung|galaxy|win|smartphone|smartwatch|gsma|android|tablet|nova|cloud|p40|camera|review|router|battery|wallet|dlink|gb|modem|notifications\", na=False, case=False)]\n",
    "print(len(combined))\n",
    "\n",
    "# Remove stop words.\n",
    "# combined['cleaned_text'] = combined['cleaned_text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
    "\n",
    "# Create month timestamp.\n",
    "combined[\"month\"] = combined['created_at'].str[:7]\n",
    "\n",
    "# Cleaning function. Add as required.\n",
    "def cleaner (pre):\n",
    "    pre['cleaned_text'] = pre['cleaned_text'].str.replace('&amp;','and')\n",
    "    pre['cleaned_text'] = pre['cleaned_text'].str.replace('&','and')\n",
    "    pre['cleaned_text'] = pre['cleaned_text'].str.replace('\\n',' ')\n",
    "    pre['cleaned_text'] = pre['cleaned_text'].str.replace('RT','')\n",
    "    pre['cleaned_text'] = pre['cleaned_text'].str.replace('http\\S+|t.co\\S+', '', case=False)\n",
    "    pre['cleaned_text'] = pre['cleaned_text'].str.replace('s://', '', case=False)\n",
    "    pre['cleaned_text'] = pre['cleaned_text'].str.replace('=andgt;', ' ', case=False)\n",
    "    pre['cleaned_text'] = pre['cleaned_text'].str.replace('andgt;', ' ', case=False)\n",
    "    pre['cleaned_text'] = pre['cleaned_text'].str.replace('http','')\n",
    "    pre['cleaned_text'] = pre['cleaned_text'].str.replace('https','')\n",
    "\n",
    "cleaner(combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22758f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract tweets from locations.\n",
    "aus = combined[combined['profile_loc'].str.contains(\"australia|sydney|canberra|melbourne|brisbane\", na=False, case=False)]\n",
    "uk = combined[combined['profile_loc'].str.contains(\"united kingdom|england|london\", na=False, case=False)]\n",
    "nz = combined[combined['profile_loc'].str.contains(\"new zealand|auckland|wellington|christchurch\", na=False, case=False)]\n",
    "usa = combined[combined['profile_loc'].str.contains(\"usa|united states|america|washington|california|new york|seattle\", na=False, case=False)]\n",
    "can = combined[combined['profile_loc'].str.contains(\"canada|ontario|toronto|british columbia|ottawa\", na=False, case=False)]\n",
    "\n",
    "print(len(aus))\n",
    "print(len(uk))\n",
    "print(len(nz))\n",
    "print(len(usa))\n",
    "print(len(can))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f30e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To make things reproducible and consistent with others.\n",
    "umap_model = UMAP(random_state=42)\n",
    "\n",
    "# To remove stop words after clustering.\n",
    "vectorizer_model = CountVectorizer(stop_words=\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964c29f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Restrict to six months before Aus's ban (2018-01-11) and the day of Canada's ban (2022-05-14) as requested by Jon.\n",
    "\n",
    "# nz[\"created_at\"] = pd.to_datetime(nz[\"created_at\"])\n",
    "# nz['date'] = nz['created_at'].dt.date\n",
    "# nz = nz.sort_values(\"date\")\n",
    "# start = date.fromisoformat('2018-01-10')\n",
    "# end = date.fromisoformat('2022-05-15')\n",
    "# print(len(nz))\n",
    "# nz = nz[(nz['date'] >= start) & (nz['date'] <= end)]\n",
    "# print(len(nz))\n",
    "\n",
    "# # Run topic model.\n",
    "# nz_data = nz.cleaned_text.to_list()\n",
    "# nz_model = BERTopic(verbose=True, n_gram_range=(1, 3), min_topic_size=10, nr_topics=30, umap_model=umap_model, top_n_words=20, vectorizer_model=vectorizer_model)\n",
    "# nz_topics, nz_probs = nz_model.fit_transform(nz_data)\n",
    "\n",
    "# # Run code to create CSV with topics. \n",
    "# nz = concat_topics('NZ', nz, nz_model)\n",
    "# nz = nz.drop(columns=['Unnamed: 0.1', 'Unnamed: 0_x', 'Unnamed: 0_y', 'Unnamed: 0_x.1', 'Unnamed: 0_y.1', 'edit_history_tweet_ids_y', 'month'], errors='ignore')\n",
    "# nz.to_csv(\"nz-tweets-with-country-and-topics.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14d1ddd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Function to select subset of data for each country based on specific date. \n",
    "def select_subset(date_of_ban, country):\n",
    "\n",
    "    # Used for one month timesteps.\n",
    "    month = relativedelta(months=1)\n",
    "    \n",
    "    # Format date. \n",
    "    d = date.fromisoformat(date_of_ban)\n",
    "    \n",
    "    # Formats date.\n",
    "    country[\"created_at\"] = pd.to_datetime(country[\"created_at\"])\n",
    "    country['date'] = country['created_at'].dt.date\n",
    "    print(len(country))\n",
    "\n",
    "    # Only includes 6 months before ban in country.\n",
    "    country = country[(country['date'] >= d-month*6) & (country['date'] <= d)]\n",
    "    print(d)\n",
    "\n",
    "    # Setting multiple conditions with one month intervals. \n",
    "    conditions = [\n",
    "        (country.date >= country.date.min()) & (country.date <= country.date.min() + month),\n",
    "        (country.date >= country.date.min() + month) & (country.date <= country.date.min() + month*2),\n",
    "        (country.date >= country.date.min() + month*2) & (country.date <= country.date.min() + month*3),\n",
    "        (country.date >= country.date.min() + month*3) & (country.date <= country.date.min() + month*4),\n",
    "        (country.date >= country.date.min() + month*4) & (country.date <= country.date.min() + month*5),\n",
    "        (country.date >= country.date.min() + month*5) & (country.date <= country.date.min() + month*6)]\n",
    "\n",
    "    # Based sequentially on the choices above. \n",
    "    choices_cat = ['m1','m2','m3','m4','m5','m6']\n",
    "\n",
    "    # Using month so BertTopic runs, although incorrect.\n",
    "    choices_month = ['2020-01-01','2020-02-01','2020-03-01','2020-04-01','2020-05-01','2020-06-01']\n",
    "\n",
    "    # Categorise.\n",
    "    country['m_cat'] = np.select(conditions, choices_cat, default=None)\n",
    "    country['month_cat'] = np.select(conditions, choices_month, default=None)\n",
    "    # country[\"month_cat\"] = pd.to_numeric(country[\"month_cat\"])\n",
    "    return country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879bf793",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dates are date of ban minus one day.\n",
    "aus = select_subset('2018-07-10', aus)\n",
    "usa = select_subset('2019-05-14', usa)\n",
    "uk = select_subset('2020-07-13', uk)\n",
    "can = select_subset('2022-05-13', can)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8baaef3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(aus))\n",
    "print(len(uk))\n",
    "print(len(usa))\n",
    "print(len(can))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4821c09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run BERTopic for each country.\n",
    "\n",
    "# Aus\n",
    "aus_data = aus.cleaned_text.to_list()\n",
    "aus_model = BERTopic(verbose=True, n_gram_range=(1, 3), min_topic_size=10, nr_topics=30, umap_model=umap_model, top_n_words=20, vectorizer_model=vectorizer_model)\n",
    "aus_topics, aus_probs = aus_model.fit_transform(aus_data)\n",
    "\n",
    "# Can\n",
    "can_data = can.cleaned_text.to_list()\n",
    "can_model = BERTopic(verbose=True, n_gram_range=(1, 3), min_topic_size=10, nr_topics=30, umap_model=umap_model, top_n_words=20, vectorizer_model=vectorizer_model)\n",
    "can_topics, can_probs = can_model.fit_transform(can_data)\n",
    "\n",
    "# USA\n",
    "usa_data = usa.cleaned_text.to_list()\n",
    "usa_model = BERTopic(verbose=True, n_gram_range=(1, 3), min_topic_size=10, nr_topics=30, umap_model=umap_model, top_n_words=20, vectorizer_model=vectorizer_model)\n",
    "usa_topics, usa_probs = usa_model.fit_transform(usa_data)\n",
    "\n",
    "# UK\n",
    "uk_data = uk.cleaned_text.to_list()\n",
    "uk_model = BERTopic(verbose=True, n_gram_range=(1, 3), min_topic_size=10, nr_topics=30, umap_model=umap_model, top_n_words=20, vectorizer_model=vectorizer_model)\n",
    "uk_topics, uk_probs = uk_model.fit_transform(uk_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1989ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Topics over time.\n",
    "# # aus_timestamps = aus.month_cat.to_list()\n",
    "# # aus_topics_over_time = aus_model.topics_over_time(aus_data, aus_topics, aus_timestamps, datetime_format=\"%Y-%m-%d\")\n",
    "# # aus_model.visualize_topics_over_time(aus_topics_over_time, top_n_topics=20)\n",
    "\n",
    "# # Note that when using CountVectorizer, some documents/topics will be empty over time, causing the aus_model viz to throw an error. \n",
    "# aus_timestamps = aus.month_cat.to_list()\n",
    "# aus_model.topics_over_time(aus_data, aus_timestamps)\n",
    "# aus_model.visualize_topics_over_time(aus_topics_over_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591b8892",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def concat_topics(name, country, model):\n",
    "    # For each country, creates column with topic numbers, i.e., -1.\n",
    "    country['topic'] = model.topics_\n",
    "    \n",
    "    # For each country, creates column with country, i.e., AU.\n",
    "    country['country_cat'] = name\n",
    "    \n",
    "    # For each country, gets a list of keywords describing each topic. \n",
    "    get_topics = pd.DataFrame(model.get_topics().items())\n",
    "    get_topics = get_topics.rename(columns = {0:'topic', 1:'keywords'})\n",
    "    \n",
    "    # For each country, gets topic definition by keywords.\n",
    "    topic_info = model.get_topic_info()\n",
    "    topic_info = topic_info.rename(columns = {'Topic':'topic'})\n",
    "    \n",
    "    # Merges into one DF. \n",
    "    country = pd.merge(country, topic_info, how='left', on='topic')\n",
    "    country = pd.merge(country, get_topics, how='left', on='topic')\n",
    "    return country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d596849c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "aus = concat_topics('AUS', aus, aus_model)\n",
    "usa = concat_topics('USA', usa, usa_model)\n",
    "can = concat_topics('CAN', can, can_model)\n",
    "uk = concat_topics('UK', uk, uk_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d210b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "aus = aus.drop(columns=['Unnamed: 0.1', 'Unnamed: 0_x', 'Unnamed: 0_y', 'Unnamed: 0_x.1', 'Unnamed: 0_y.1', 'edit_history_tweet_ids_y', 'month'], errors='ignore')\n",
    "can = can.drop(columns=['Unnamed: 0.1', 'Unnamed: 0_x', 'Unnamed: 0_y', 'Unnamed: 0_x.1', 'Unnamed: 0_y.1', 'edit_history_tweet_ids_y', 'month'], errors='ignore')\n",
    "uk = uk.drop(columns=['Unnamed: 0.1', 'Unnamed: 0_x', 'Unnamed: 0_y', 'Unnamed: 0_x.1', 'Unnamed: 0_y.1', 'edit_history_tweet_ids_y', 'month'], errors='ignore')\n",
    "usa = usa.drop(columns=['Unnamed: 0.1', 'Unnamed: 0_x', 'Unnamed: 0_y', 'Unnamed: 0_x.1', 'Unnamed: 0_y.1', 'edit_history_tweet_ids_y', 'month'], errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1be0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = pd.concat([aus,can,uk,usa])\n",
    "len(combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62671779",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined.to_csv(\"tweets-with-country-and-topics.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bert",
   "language": "python",
   "name": "bert"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
